{"status":"ok","feed":{"url":"https://medium.com/feed/@jiangjs03","title":"Stories by Ian Jiang on Medium","link":"https://medium.com/@jiangjs03?source=rss-1e3d40487e56------2","author":"","description":"Stories by Ian Jiang on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/0*nhdeuwUZ_izofP25"},"items":[{"title":"The Quest for True AI Reasoning: Data-Driven Approaches and Extreme Hardware Optimization","pubDate":"2025-02-21 01:45:20","link":"https://medium.com/@jiangjs03/the-quest-for-true-ai-reasoning-data-driven-approaches-and-extreme-hardware-optimization-1797a4b0bf47?source=rss-1e3d40487e56------2","guid":"https://medium.com/p/1797a4b0bf47","author":"Ian Jiang","thumbnail":"","description":"\n<h3>Abstract</h3>\n<p><strong>During the initial emergence of the large language models, skepticism arose concerning whether or not these models truly \u201cunderstood\u201d or simply existed as surface-level fakes of thought processes. Recent developments by way of DeepSeek have brought forth evidence that real reasoning can spring from careful data-driven methods-with hardware-level optimizations. This paper will discuss how DeepSeek rubs salt into the wound of the philosophy of \u201cfake thinking.\u201d It will discuss how it envisions fine-grained training data together with low-level customizations at the PTX layer of NVIDIA GPUs. It will also discuss how this type of approach can benefit resource-constrained environments, forecasting the prospect of AI reasoning advancing hand in hand with hardware innovation. Empirical data and benchmark results are discussed relative to the claims\u00a0made.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WDUm4V5smJ3V6Z9cF93u0w.png\"></figure><h3>\u2160. Introduction</h3>\n<p><strong>Since the introduction of the ChatGPT o1 model, discussions have ensued among scholars and practitioners on whether these models really \u201cthink\u201d or whether they are just disguises put on the action of repeating texts in an articulate way and are more likely to generate more correct \u201canswers\u201d through constant critique and negation. Critics call those language models \u201cthinking\u201d or just repeating the text in a seemingly intelligent manner. However, with the introduction of increasingly sophisticated data-driven training methods, particularly DeepSeek\u2019s approach, the emerging evidence seems to be growing to show that LLMs can indeed be directed toward deeper forms of reasoning.</strong></p>\n<p><strong>Achieving genuine reasoning relies primarily on the capability of effective representations of logical chains and the efficiency offered by hardware. Loosely speaking, two areas help in this objective: Software utilizes technology to reduce computational needs while aiming for desired performance. Also on the performance side, state-of-the-art optimizations of Parallel Thread Execution (PTX) tend to drive GPUs to their limits. The DeepSeek approach insists on extreme performance use and low-cost efficacy that have turned out to be game-changers for market competitors. Google is also moving toward the following Gemini Think Model, and word is that the new Gemini\u2019s model cost generation with tokens might turn out to be just about that of DeepSeek. Recent studies have shown that models trained with structured logic chains displayed measurable improvements on reasoning-based tasks, such as GSM8K and ARC benchmarks, achieving up to a 20% increase in accuracy compared to traditional models (Hendrycks et al.,\u00a02021).</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/535/1*kjU-aw-LrBC_yi8YYhZJzw.png\"></figure><h3>\u2161. AI Training Optimization: The Next Technological Frontier</h3>\n<p><strong>While LLMs are developing, optimizing AI training is the next big thing. Conventional models are trained using huge computational resources and massively large datasets. However, large-scale models cannot be constructed using this approach in future days due to their exponentially growing size. Hence, technical researchers have begun to devise more resource-efficient training mechanisms that could produce high-performance models while addressing the computational slew.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rtuS9V_Oo3C3uc9g2N_7zQ.png\"></figure><h4>Low-Rank Adaptation (LoRA)</h4>\n<p><strong>LoRA cuts down on computational and memory expense by selectively fine-tuning a subset of model parameters. Indeed, research has shown that the LoRA method can achieve a 70% reduction in GPU memory costs relative to 95% of original accuracy (Hu et al., 2021). This allows efficient LLM fine-tuning on characteristically resource-constrained devices. Cost-efficient fine-tuning relies on the selection of the most significant subsets of parameters for low-rank matrix adaptation, which allows for some trade-off between keeping accuracy and achieving computational efficiency. More and more language tools coming out like\u00a0Bend.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/356/1*h7N0cRercr-Z7JZ3EHokmw.png\"></figure><h4>Distributed Training Optimization</h4>\n<p><strong>There\u2019s no chance one can continue training a big model on a single machine. Distributed training distributes workloads among various GPUs or servers to considerably increase compute efficiency, but the problems of communication overhead and synchronization still stand out. Research done by NVIDIA shows that identical techniques in optimization could reduce the communication overhead by 30\u201340% (NVIDIA, 2023). Techniques including gradient compression and parameter sharding further bolster efficiency by reducing memory overhead and boosting parallelism.</strong></p>\n<h4>Sparse Training and Quantization</h4>\n<p><strong>Sparse training and quantization reduce model parameter density or lower parameter precision (decrease from 32-bit floating-point to 8-bit integer) so significantly that computational and storage requirements are cut by a large fraction. In inference, Google\u2019s researches indicate that quantization techniques may offer speed-ups of about X2 achieve to X3 with an accuracy reduction less than 1% (Jacob et al. 2018). Some recent advances in structured pruning methods pose the great possibility of allowing for sparse training without significant accuracy penalty, and thus allow for real-time deployment to be feasible on low-power edge\u00a0devices.</strong></p>\n<h3>\u2162. Thinking Models and Hardware Evolution</h3>\n<p><strong>With the coming of thought math (like the DeepSeek proposal), it is a paradigm shift from just recognizing patterns to structured logical reasoning. Those incorporate logical structured chains and reasoning in steps to enhance substantially the performance for complex tasks. However, it creates added demands on hardware\u00a0fabric.</strong></p>\n<h4>High-Performance Computing Clusters</h4>\n<p><strong>The larger model training and inference workloads require the more enhanced formation of high-performance computing (HPC) clusters in data centers. These incorporate advanced cooling methods on the energy-efficient designs which automatically lower operating costs. Reported to have achieved 2.5x throughput on large models against traditional GPU clusters, Google\u2019s TPU Pods provide compelling evidence (Jouppi et al.,\u00a02020).</strong></p>\n<h4>Specialized AI\u00a0Chips</h4>\n<p><strong>The general-purpose processors such as CPUs and GPUs might not be sufficient in bearing burdens induced by the ever-growing AI workloads. Special-purpose chips, including TPUs and advanced NPUs, deliver a higher efficiency while consuming less energy. The Dojo chip made by Tesla for autonomous-driving AI is rumored to be ten times faster than conventional GPUs (Tesla, 2023). AI-based hardware design integration into training accelerators is one particular trend shaping next-generation architectures.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/624/1*yzlYzYwgVJJ3lEQTSn6D_A.png\"></figure><h4>Edge Computing Devices</h4>\n<p><strong>As AI applications expand their domain to include edge devices, future computing infrastructure must enable distributed computing and edge collaboration. Studies show edge computing reduces latency by more than 50% and decreases bandwidth requirements (Satyanarayanan, 2017). Application examples are intelligent home assistants, real-time traffic analysis in autonomous vehicles, and medical diagnosis systems based on localized AI inference.</strong></p>\n<h3>\u2163. Conclusion</h3>\n<p><strong>The changeover from \u201cfake thinking\u201d to real logical reasoning continues at all levels in AI. Organizations like DeepSeek are proving that logic-based training methodologies and specific hardware optimizations can bring AI closer to true reasoning. The entire transition between AI-based training optimizations and specialized hardware is preparing for the next frontier of AI development. In the coming shift, AI is bound to alter its computational basis from the almost complete dominance of GPUs and TPUs to a concentration in neuromorphic and optical computing. Some future research directions will prioritize AI-based PTX code generation, multi-agent collaborative inference, and new AI-optimized hardware architectures, such as neuromorphic and optical computing.</strong></p>\n<h3>References</h3>\n<p>\u25cf <strong>Bubeck, S\u00e9bastien, et al. 2023. \u201cSparks of Artificial General Intelligence: Early Experiments with GPT-4.\u201d arXiv preprint arXiv:2303.12712.</strong></p>\n<p>\u25cf <strong>Cai, Han, et al. 2019. \u201cOnce-for-All: Train One Network and Specialize It for Efficient Deployment.\u201d arXiv preprint arXiv:1908.09791.</strong></p>\n<p>\u25cf <strong>Finn, Chelsea, et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d arXiv preprint arXiv:1703.03400.</strong></p>\n<p>\u25cf <strong>Hu, Edward J., et al. 2021. \u201cLoRA: Low-Rank Adaptation of Large Language Models.\u201d arXiv preprint arXiv:2106.09685.</strong></p>\n<p>\u25cf <strong>Jacob, Benoit, et al. 2018. \u201cQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.\u201d CVPR.</strong></p>\n<p>\u25cf <strong>Jouppi, Norman P., et al. 2020. \u201cTPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.\u201d arXiv preprint arXiv:2003.02314.</strong></p>\n<p>\u25cf <strong>Lake, Brenden M., et al. 2017. \u201cBuilding Machines That Learn and Think Like People.\u201d Behavioral and Brain Sciences.</strong></p>\n<p>\u25cf <strong>Satyanarayanan, Mahadev. 2017. \u201cThe Emergence of Edge Computing.\u201d Computer.</strong></p>\n<p>\u25cf <strong>Tesla. 2023. \u201cDojo: Tesla\u2019s Custom AI Chip for Autonomous Driving.\u201d Tesla AI Day Presentation.</strong></p>\n<p>\u25cf <strong>NVIDIA. 2023. \u201cDistributed Training Optimization Techniques.\u201d NVIDIA Developer Blog.</strong></p>\n<p>\u25cf <strong>NVIDIA. 2024. \u201cCUDA Parallel Thread Execution ISA.\u201d NVIDIA Developer Documentation.</strong></p>\n<p>\u25cf <strong>Hu, Edward J., et al. 2021. \u201cLoRA: Low-Rank Adaptation of Large Language Models.\u201d <em>arXiv preprint</em> arXiv:2106.09685.</strong></p>\n<p>\u25cf <strong>OpenLM. 2024. \u201cDeepSeek-V2 Model Architecture.\u201d <em>OpenLM.ai</em>.</strong></p>\n<p>\u25cf <strong>NVIDIA. 2020. \u201cNVIDIA Ampere Architecture: TensorFloat-32 (TF32) for Deep Learning and HPC.\u201d NVIDIA Technical Blog.<br></strong><a href=\"https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\"><strong>https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/</strong></a></p>\n<p>\u25cf <strong>Lightning AI. 2023. \u201cParameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA).\u201d Lightning AI Community.<br></strong><a href=\"https://lightning.ai/pages/community/article/lora-llm/\"><strong>https://lightning.ai/pages/community/article/lora-llm/</strong></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1797a4b0bf47\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Abstract</h3>\n<p><strong>During the initial emergence of the large language models, skepticism arose concerning whether or not these models truly \u201cunderstood\u201d or simply existed as surface-level fakes of thought processes. Recent developments by way of DeepSeek have brought forth evidence that real reasoning can spring from careful data-driven methods-with hardware-level optimizations. This paper will discuss how DeepSeek rubs salt into the wound of the philosophy of \u201cfake thinking.\u201d It will discuss how it envisions fine-grained training data together with low-level customizations at the PTX layer of NVIDIA GPUs. It will also discuss how this type of approach can benefit resource-constrained environments, forecasting the prospect of AI reasoning advancing hand in hand with hardware innovation. Empirical data and benchmark results are discussed relative to the claims\u00a0made.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WDUm4V5smJ3V6Z9cF93u0w.png\"></figure><h3>\u2160. Introduction</h3>\n<p><strong>Since the introduction of the ChatGPT o1 model, discussions have ensued among scholars and practitioners on whether these models really \u201cthink\u201d or whether they are just disguises put on the action of repeating texts in an articulate way and are more likely to generate more correct \u201canswers\u201d through constant critique and negation. Critics call those language models \u201cthinking\u201d or just repeating the text in a seemingly intelligent manner. However, with the introduction of increasingly sophisticated data-driven training methods, particularly DeepSeek\u2019s approach, the emerging evidence seems to be growing to show that LLMs can indeed be directed toward deeper forms of reasoning.</strong></p>\n<p><strong>Achieving genuine reasoning relies primarily on the capability of effective representations of logical chains and the efficiency offered by hardware. Loosely speaking, two areas help in this objective: Software utilizes technology to reduce computational needs while aiming for desired performance. Also on the performance side, state-of-the-art optimizations of Parallel Thread Execution (PTX) tend to drive GPUs to their limits. The DeepSeek approach insists on extreme performance use and low-cost efficacy that have turned out to be game-changers for market competitors. Google is also moving toward the following Gemini Think Model, and word is that the new Gemini\u2019s model cost generation with tokens might turn out to be just about that of DeepSeek. Recent studies have shown that models trained with structured logic chains displayed measurable improvements on reasoning-based tasks, such as GSM8K and ARC benchmarks, achieving up to a 20% increase in accuracy compared to traditional models (Hendrycks et al.,\u00a02021).</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/535/1*kjU-aw-LrBC_yi8YYhZJzw.png\"></figure><h3>\u2161. AI Training Optimization: The Next Technological Frontier</h3>\n<p><strong>While LLMs are developing, optimizing AI training is the next big thing. Conventional models are trained using huge computational resources and massively large datasets. However, large-scale models cannot be constructed using this approach in future days due to their exponentially growing size. Hence, technical researchers have begun to devise more resource-efficient training mechanisms that could produce high-performance models while addressing the computational slew.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rtuS9V_Oo3C3uc9g2N_7zQ.png\"></figure><h4>Low-Rank Adaptation (LoRA)</h4>\n<p><strong>LoRA cuts down on computational and memory expense by selectively fine-tuning a subset of model parameters. Indeed, research has shown that the LoRA method can achieve a 70% reduction in GPU memory costs relative to 95% of original accuracy (Hu et al., 2021). This allows efficient LLM fine-tuning on characteristically resource-constrained devices. Cost-efficient fine-tuning relies on the selection of the most significant subsets of parameters for low-rank matrix adaptation, which allows for some trade-off between keeping accuracy and achieving computational efficiency. More and more language tools coming out like\u00a0Bend.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/356/1*h7N0cRercr-Z7JZ3EHokmw.png\"></figure><h4>Distributed Training Optimization</h4>\n<p><strong>There\u2019s no chance one can continue training a big model on a single machine. Distributed training distributes workloads among various GPUs or servers to considerably increase compute efficiency, but the problems of communication overhead and synchronization still stand out. Research done by NVIDIA shows that identical techniques in optimization could reduce the communication overhead by 30\u201340% (NVIDIA, 2023). Techniques including gradient compression and parameter sharding further bolster efficiency by reducing memory overhead and boosting parallelism.</strong></p>\n<h4>Sparse Training and Quantization</h4>\n<p><strong>Sparse training and quantization reduce model parameter density or lower parameter precision (decrease from 32-bit floating-point to 8-bit integer) so significantly that computational and storage requirements are cut by a large fraction. In inference, Google\u2019s researches indicate that quantization techniques may offer speed-ups of about X2 achieve to X3 with an accuracy reduction less than 1% (Jacob et al. 2018). Some recent advances in structured pruning methods pose the great possibility of allowing for sparse training without significant accuracy penalty, and thus allow for real-time deployment to be feasible on low-power edge\u00a0devices.</strong></p>\n<h3>\u2162. Thinking Models and Hardware Evolution</h3>\n<p><strong>With the coming of thought math (like the DeepSeek proposal), it is a paradigm shift from just recognizing patterns to structured logical reasoning. Those incorporate logical structured chains and reasoning in steps to enhance substantially the performance for complex tasks. However, it creates added demands on hardware\u00a0fabric.</strong></p>\n<h4>High-Performance Computing Clusters</h4>\n<p><strong>The larger model training and inference workloads require the more enhanced formation of high-performance computing (HPC) clusters in data centers. These incorporate advanced cooling methods on the energy-efficient designs which automatically lower operating costs. Reported to have achieved 2.5x throughput on large models against traditional GPU clusters, Google\u2019s TPU Pods provide compelling evidence (Jouppi et al.,\u00a02020).</strong></p>\n<h4>Specialized AI\u00a0Chips</h4>\n<p><strong>The general-purpose processors such as CPUs and GPUs might not be sufficient in bearing burdens induced by the ever-growing AI workloads. Special-purpose chips, including TPUs and advanced NPUs, deliver a higher efficiency while consuming less energy. The Dojo chip made by Tesla for autonomous-driving AI is rumored to be ten times faster than conventional GPUs (Tesla, 2023). AI-based hardware design integration into training accelerators is one particular trend shaping next-generation architectures.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/624/1*yzlYzYwgVJJ3lEQTSn6D_A.png\"></figure><h4>Edge Computing Devices</h4>\n<p><strong>As AI applications expand their domain to include edge devices, future computing infrastructure must enable distributed computing and edge collaboration. Studies show edge computing reduces latency by more than 50% and decreases bandwidth requirements (Satyanarayanan, 2017). Application examples are intelligent home assistants, real-time traffic analysis in autonomous vehicles, and medical diagnosis systems based on localized AI inference.</strong></p>\n<h3>\u2163. Conclusion</h3>\n<p><strong>The changeover from \u201cfake thinking\u201d to real logical reasoning continues at all levels in AI. Organizations like DeepSeek are proving that logic-based training methodologies and specific hardware optimizations can bring AI closer to true reasoning. The entire transition between AI-based training optimizations and specialized hardware is preparing for the next frontier of AI development. In the coming shift, AI is bound to alter its computational basis from the almost complete dominance of GPUs and TPUs to a concentration in neuromorphic and optical computing. Some future research directions will prioritize AI-based PTX code generation, multi-agent collaborative inference, and new AI-optimized hardware architectures, such as neuromorphic and optical computing.</strong></p>\n<h3>References</h3>\n<p>\u25cf <strong>Bubeck, S\u00e9bastien, et al. 2023. \u201cSparks of Artificial General Intelligence: Early Experiments with GPT-4.\u201d arXiv preprint arXiv:2303.12712.</strong></p>\n<p>\u25cf <strong>Cai, Han, et al. 2019. \u201cOnce-for-All: Train One Network and Specialize It for Efficient Deployment.\u201d arXiv preprint arXiv:1908.09791.</strong></p>\n<p>\u25cf <strong>Finn, Chelsea, et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d arXiv preprint arXiv:1703.03400.</strong></p>\n<p>\u25cf <strong>Hu, Edward J., et al. 2021. \u201cLoRA: Low-Rank Adaptation of Large Language Models.\u201d arXiv preprint arXiv:2106.09685.</strong></p>\n<p>\u25cf <strong>Jacob, Benoit, et al. 2018. \u201cQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.\u201d CVPR.</strong></p>\n<p>\u25cf <strong>Jouppi, Norman P., et al. 2020. \u201cTPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.\u201d arXiv preprint arXiv:2003.02314.</strong></p>\n<p>\u25cf <strong>Lake, Brenden M., et al. 2017. \u201cBuilding Machines That Learn and Think Like People.\u201d Behavioral and Brain Sciences.</strong></p>\n<p>\u25cf <strong>Satyanarayanan, Mahadev. 2017. \u201cThe Emergence of Edge Computing.\u201d Computer.</strong></p>\n<p>\u25cf <strong>Tesla. 2023. \u201cDojo: Tesla\u2019s Custom AI Chip for Autonomous Driving.\u201d Tesla AI Day Presentation.</strong></p>\n<p>\u25cf <strong>NVIDIA. 2023. \u201cDistributed Training Optimization Techniques.\u201d NVIDIA Developer Blog.</strong></p>\n<p>\u25cf <strong>NVIDIA. 2024. \u201cCUDA Parallel Thread Execution ISA.\u201d NVIDIA Developer Documentation.</strong></p>\n<p>\u25cf <strong>Hu, Edward J., et al. 2021. \u201cLoRA: Low-Rank Adaptation of Large Language Models.\u201d <em>arXiv preprint</em> arXiv:2106.09685.</strong></p>\n<p>\u25cf <strong>OpenLM. 2024. \u201cDeepSeek-V2 Model Architecture.\u201d <em>OpenLM.ai</em>.</strong></p>\n<p>\u25cf <strong>NVIDIA. 2020. \u201cNVIDIA Ampere Architecture: TensorFloat-32 (TF32) for Deep Learning and HPC.\u201d NVIDIA Technical Blog.<br></strong><a href=\"https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\"><strong>https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/</strong></a></p>\n<p>\u25cf <strong>Lightning AI. 2023. \u201cParameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA).\u201d Lightning AI Community.<br></strong><a href=\"https://lightning.ai/pages/community/article/lora-llm/\"><strong>https://lightning.ai/pages/community/article/lora-llm/</strong></a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1797a4b0bf47\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["hardware","ai","machine-learning","lora"]}]}